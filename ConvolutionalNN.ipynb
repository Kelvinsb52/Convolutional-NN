{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the MNIST Dataset: Classification (Digits 0 and 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    # Reshape and normalize input data\n",
    "    x = x.reshape(x.shape[0], 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    # Encode output which is a number in range [0,9] into a vector of size 10\n",
    "    y = to_categorical(y)  \n",
    "    y = y.reshape(y.shape[0], 10, 1)\n",
    "    return x[:limit], y[:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Mnist dataset as is from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel values of first x_train image: \n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
      "  175  26 166 255 247 127   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
      "  225 172 253 242 195  64   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
      "   93  82  82  56  39   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
      "   25   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
      "  150  27   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
      "  253 187   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
      "  253 249  64   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
      "  253 207   2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
      "  250 182   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
      "   78   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pixel values of first x_train image: \\n{x_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True category of first example: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"True category of first example: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST from server, limit to 100 images per class since we're not training on GPU\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 1000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1, 28, 28), (1000, 10, 1))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel values of first x_train image: \n",
      "[[[0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.01176471 0.07058824 0.07058824 0.07058824 0.49411765 0.53333336\n",
      "   0.6862745  0.10196079 0.6509804  1.         0.96862745 0.49803922\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      "   0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      "   0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.19215687 0.93333334 0.99215686 0.99215686 0.99215686\n",
      "   0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.9843137\n",
      "   0.3647059  0.32156864 0.32156864 0.21960784 0.15294118 0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.07058824 0.85882354 0.99215686 0.99215686 0.99215686\n",
      "   0.99215686 0.99215686 0.7764706  0.7137255  0.96862745 0.94509804\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      "   0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.05490196 0.00392157 0.6039216\n",
      "   0.99215686 0.3529412  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.54509807\n",
      "   0.99215686 0.74509805 0.00784314 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.04313726\n",
      "   0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.13725491 0.94509804 0.88235295 0.627451   0.42352942 0.00392157\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.31764707 0.9411765  0.99215686 0.99215686 0.46666667\n",
      "   0.09803922 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      "   0.5882353  0.10588235 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.0627451  0.3647059  0.9882353\n",
      "   0.99215686 0.73333335 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.9764706\n",
      "   0.99215686 0.9764706  0.2509804  0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      "   0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.15294118 0.5803922  0.8980392  0.99215686 0.99215686 0.99215686\n",
      "   0.98039216 0.7137255  0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.09411765 0.44705883\n",
      "   0.8666667  0.99215686 0.99215686 0.99215686 0.99215686 0.7882353\n",
      "   0.30588236 0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      "   0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.07058824 0.67058825 0.85882354 0.99215686 0.99215686 0.99215686\n",
      "   0.99215686 0.7647059  0.3137255  0.03529412 0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.21568628 0.6745098\n",
      "   0.8862745  0.99215686 0.99215686 0.99215686 0.99215686 0.95686275\n",
      "   0.52156866 0.04313726 0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.53333336 0.99215686\n",
      "   0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.         0.         0.\n",
      "   0.         0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"pixel values of first x_train image: \\n{x_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot encoded array of first y_train image: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"one-hot encoded array of first y_train image: \\n{y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I need a function that performs prediction on my neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs prediction \n",
    "def predict(network, input, training):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output, training)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I also need a function that will perform forward propogation, calculate loss, backpropogate and update weights to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, loss, loss_prime, x_train, y_train, epochs, learning_rate, verbose=True):\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train): # Will iterate over each example\n",
    "            # Forward pass on neural network\n",
    "            output = predict(network, x, True)\n",
    "\n",
    "            # Accumulate error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # Backward pass on neural network\n",
    "            grad = loss_prime(y, output) # Derivate of loss with respect to prediction of neural network output\n",
    "            for layer in reversed(network): # Start from last layer to perform back propogation\n",
    "                grad = layer.backward(grad, learning_rate) # calculating gradient of each layer and stepping when needed in each layer\n",
    "\n",
    "        # Calculate average error over training set and print per epoch\n",
    "        error /= len(x_train) \n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing a Base Layer for Neural Networks\n",
    "\n",
    "Before writing the code for any layers, I will start off with a base layer that will define the common structure and interface (forward and backward methods) for all layers. This will ensure consistency, modularity, and reusability across different types of layers in the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input, training):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The main star of the show: The Convolutional Layer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolutional(Layer):\n",
    "    # Input shape = depth x height x width\n",
    "    # kernel_size = size of each matrix of each kernel\n",
    "    # depth = number of kernels\n",
    "    # input depth is different than depth\n",
    "    def __init__(self, input_shape, kernel_size, depth):\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.depth = depth\n",
    "        self.input_shape = input_shape\n",
    "        self.input_depth = input_depth\n",
    "        # In this convolutional layer the formula to calculate output shape is [depth, (input size - kernel size) + 1] \n",
    "        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1) \n",
    "        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size) # (Number of kernels X input depth X kernel size X kernel size)\n",
    "        self.kernels = np.random.randn(*self.kernels_shape) # 4d matrix of the weights\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, input, training):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.depth): # Iterate up to number of kernel times\n",
    "            for j in range(self.input_depth): # Iterate up to input depth times\n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernels[i, j], \"valid\") # Calculate valid cross correlation between single input image and kernel\n",
    "        return self.output # This will have number of kernels amount of output matrices\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        kernels_gradient = np.zeros(self.kernels_shape)\n",
    "        input_gradient = np.zeros(self.input_shape)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            for j in range(self.input_depth):\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets break down the math forward pass first\n",
    "``` \n",
    "def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.depth): \n",
    "            for j in range(self.input_depth): \n",
    "                self.output[i] += signal.correlate2d(self.input[j], self.kernel[i, j], \"valid\") \n",
    "        return self.output \n",
    "``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Math Behind Valid Cross-Correlation with Matrices\n",
    "\n",
    "This function operates on a given matrix: $I_{n \\times n}$. The matrix $I$ also includes a color channel dimension, but since the channel size is 1, we simplify it as $n \\times n$ for simplicity.\n",
    "\n",
    "Additionally, there is another matrix called the **kernel**, denoted as $K_{m \\times m}$. The operation performed between these two matrices is called **valid cross-correlation**, represented as $I \\star K$.\n",
    "\n",
    "#### Formula for Valid Cross-Correlation\n",
    "For a matrix $A_{n \\times n}$ (input) and a matrix $B_{m \\times m}$ (kernel), the valid cross-correlation at position $(i, j)$ is defined as:\n",
    "\n",
    "$\n",
    "(A \\star B)[i, j] = \\sum_{p=0}^{m-1} \\sum_{q=0}^{m-1} A[i + p, j + q] \\cdot B[p, q]\n",
    "$\n",
    "\n",
    "Here:\n",
    "- $A$: Input matrix\n",
    "- $B$: Kernel matrix\n",
    "- $i, j$: Indices of the **output matrix**\n",
    "\n",
    "#### Output Matrix Dimensions\n",
    "Let $O$ be the output matrix such that $O$ = $(A \\star B)[i, j]$. Then the size of O is calculated as:\n",
    "$\n",
    "\\text{Output size} = (\\text{Input size} - \\text{Kernel size}) + 1\n",
    "$\n",
    "\n",
    "For example:\n",
    "- Input size: $4 \\times 4$\n",
    "- Kernel size: $2 \\times 2$\n",
    "\n",
    "$\n",
    "\\text{Output size} = (4 - 2) + 1 = 3 \\times 3\n",
    "$\n",
    "\n",
    "#### Example Calculation\n",
    "Let the input matrix $A$, kernel $B$, and output matrix $O$ be defined as:  \n",
    "$\n",
    "A = \\begin{bmatrix} \n",
    "1 & 2 & 3 & 4 \\\\ \n",
    "5 & 6 & 7 & 8 \\\\ \n",
    "9 & 10 & 11 & 12 \\\\ \n",
    "13 & 14 & 15 & 16 \n",
    "\\end{bmatrix}, \\quad\n",
    "B = \\begin{bmatrix} \n",
    "1 & -2 \\\\ \n",
    "2 & 0 \n",
    "\\end{bmatrix}, \\quad\n",
    "O = \\begin{bmatrix} \n",
    "a_{00} & a_{01} & a_{02} \\\\ \n",
    "a_{10} & a_{11} & a_{12} \\\\ \n",
    "a_{20} & a_{21} & a_{22} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "To compute $O[0, 0]$ (the output at position $i = 0, j = 0$):\n",
    "$\n",
    "O[0, 0] = \\sum_{p=0}^{1} \\sum_{q=0}^{1} A[0 + p, 0 + q] \\cdot B[p, q]\n",
    "$\n",
    "\n",
    "Step-by-step:\n",
    "1. $A_{[0, 0]} \\cdot B_{[0, 0]} = 1 \\cdot 1 = 1$\n",
    "2. $A_{[0, 1]} \\cdot B_{[0, 1]} = 2 \\cdot (-2) = -4$\n",
    "3. $A_{[1, 0]} \\cdot B_{[1, 0]} = 5 \\cdot 2 = 10$\n",
    "4. $A_{[1, 1]} \\cdot B_{[1, 1]} = 6 \\cdot 0 = 0$\n",
    "\n",
    "Sum these results:\n",
    "$\n",
    "O[0, 0] = 1 + (-4) + 10 + 0 = 7\n",
    "$\n",
    "\n",
    "#### Final Step\n",
    "Repeat this computation for all $i, j$ in the output matrix $O$.\n",
    "\n",
    "The following line of code does exactly this computation but iterates over the depth of the input(amount of color channels) $ j $ and the depth of the kernel (amount of kernels) $ i $.\n",
    "```\n",
    "self.output[i] += signal.correlate2d(self.input[j], self.kernel[i, j], \"valid\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the second star of the show: Backward Propogation on the Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main question is: **how does the loss change when I modify the parameters in the kernel?** To answer this, we must determine how each weight in each kernel impacts the loss. This requires computing the **gradient of the loss with respect to each weight in the kernel**.\n",
    "\n",
    "### Simplifying the Gradient Calculation\n",
    "\n",
    "The gradient of the kernel $K_{[i, j]}$ at position $(i, j)$ can be simplified by combining:\n",
    "1. The **gradient of the loss with respect to the output of the current layer**.\n",
    "2. The **gradient of the output with respect to the kernel weights**.\n",
    "\n",
    "In general:\n",
    "$$\n",
    "\\text{Gradient of Kernel} = \\text{Output Gradient} \\cdot \\text{Input to the Layer}.\n",
    "$$\n",
    "\n",
    "### Forward Pass Formula\n",
    "\n",
    "To understand this better, remember the formula for the forward pass of a convolution operation. For the output of a convolutional layer $Y_{[i, j]}$, we compute:\n",
    "$$\n",
    "Y_{[i, j]} = (A \\star B)[i, j] = \\sum_{p=0}^{m-1} \\sum_{q=0}^{m-1} A_{[i + p, j + q]} \\cdot B_{[p, q]}\n",
    "$$\n",
    "This expands as:\n",
    "$$\n",
    "Y_{[i, j]} = A_{[i + 0, j + 0]} \\cdot B_{[0, 0]} + A_{[i + 0, j + 1]} \\cdot B_{[0, 1]} + \\dots + A_{[i + m - 1, j + m - 1]} \\cdot B_{[m - 1, m - 1]}.\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $A$: The input to the convolutional layer.\n",
    "- $B$: The kernel.\n",
    "- $\\star$: The convolution operator.\n",
    "\n",
    "### Calculating the Gradient\n",
    "\n",
    "To compute the gradient $\\frac{\\partial Y_{[i, j]}}{\\partial B_{[p, q]}}$, consider the contribution of each kernel weight $B_{[p, q]}$ to $Y_{[i, j]}$. The derivative is:\n",
    "$$\n",
    "\\frac{\\partial Y_{[i, j]}}{\\partial B_{[p, q]}} = A_{[i + p, j + q]}.\n",
    "$$\n",
    "This result shows that changing $B_{[p, q]}$ only affects $Y_{[i, j]}$ through the corresponding input value $A_{[i + p, j + q]}$.\n",
    "\n",
    "This derivative is referred to as the **local gradient**, as it demonstrates the relationship between the kernel and the output.\n",
    "\n",
    "### Incorporating the Chain Rule\n",
    "\n",
    "To compute the full gradient of the loss with respect to $B_{[p, q]}$, we must multiply the local gradient by the gradient of the loss from the next layer:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B_{[p, q]}} = \\frac{\\partial L}{\\partial Y_{[i, j]}} \\cdot \\frac{\\partial Y_{[i, j]}}{\\partial B_{[p, q]}}.\n",
    "$$\n",
    "\n",
    "Expanding this, we have:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B_{[p, q]}} = \\frac{\\partial L_{[i, j]}}{\\partial Y_{[i, j]}} \\cdot A_{[i + p, j + q]}.\n",
    "$$\n",
    "\n",
    "This gives the full gradient of the kernel, which is then used to update the kernel weights during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input, training):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
    "    \n",
    "    def print(self, input):\n",
    "        print(f\"Activation {self.activation(input)}\")\n",
    "        print(f\"Activation prime {self.activation_prime(input)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.maximum(0 ,x)\n",
    "        \n",
    "        def relu_prime(x):\n",
    "            return (x > 0).astype(float)\n",
    "\n",
    "        super().__init__(relu, relu_prime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ReLU Activation with He Initialization\n",
    "\n",
    "This network uses the **ReLU activation function**. However, I encountered an issue where:  \n",
    "The *loss decreased very slowly*, and the *predictions were poor*.\n",
    "\n",
    "To address this, I implemented **He initialization** in my Dense layer. This initialization helps maintain stable activation variances across layers when using ReLU. Specifically, I used the scaling factor:\n",
    "\n",
    "$\n",
    "\\sqrt{\\frac{2}{\\text{input\\_size}}}\n",
    "$\n",
    "\n",
    "This factor scales the initial weights appropriately for layers with ReLU activations, ensuring better convergence and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # The scaling factor \"np.sqrt(2 / input_size)\" accounts for number of incoming connections to each neuron\n",
    "        # maintaining stability\n",
    "        self.weights = np.random.randn(output_size, input_size) *  np.sqrt(2 / input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input, training):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient # Subtracts a small step of learning rate * gradients of weights in order to minimize loss\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.weights.shape)\n",
    "        print(self.bias.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross-Entropy Loss Function\n",
    "\n",
    "Since I am predicting across **10 classes**, I chose the **categorical cross-entropy loss function**. This loss is defined as:\n",
    "\n",
    "$\n",
    "L = -\\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{i,j} \\log(\\hat{y}_{i,j})\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    "- ( N ) is the number of samples,\n",
    "- ( C ) is the number of classes,\n",
    "- $( y_{i,j} ) $ is the ground truth for the ( i )-th sample and ( j )-th class, and \n",
    "- $( \\hat{y}_{i,j} ) $ is the predicted probability for the ( i )-th sample and ( j )-th class.\n",
    "\n",
    "#### Derivative of the Loss Function\n",
    "The derivative with respect to the predicted probability $ ( \\hat{y}_{i,j} ) $ is:\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial \\hat{y}_{i,j}} = \\frac{\\hat{y}_{i,j} - y_{i,j}}{N}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "  \n",
    "    return -np.mean(np.sum(y_true * np.log(np.clip(y_pred, 1e-12, 1 - 1e-12)), axis=1))\n",
    "\n",
    "def categorical_cross_entropy_prime(y_true, y_pred):\n",
    "    \n",
    "    return (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following layers are straightforward: a **reshape layer**, followed by a **Dense** layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(Layer):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, input, training):\n",
    "        return np.reshape(input, self.output_shape)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.reshape(output_gradient, self.input_shape)\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # The scaling factor \"np.sqrt(2 / input_size)\" accounts for number of incoming connections to each neuron\n",
    "        # maintaining stability\n",
    "        self.weights = np.random.randn(output_size, input_size) *  np.sqrt(2 / input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input, training):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient # Subtracts a small step of learning rate * gradients of weights in order to minimize loss\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.weights.shape)\n",
    "        print(self.bias.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a **softmax** and **dropout** layer is used to output the probability of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, input, training):\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # This version is faster than the one presented in the video\n",
    "        n = np.size(self.output)\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)\n",
    "        # Original formula:\n",
    "        # tmp = np.tile(self.output, n)\n",
    "        # return np.dot(tmp * (np.identity(n) - np.transpose(tmp)), output_gradient)\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, rate):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, input, training):\n",
    "        if training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.rate, size=input.shape) \n",
    "            return input * self.mask / (1 - self.rate)\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return output_gradient * self.mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring and Training the Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100, error=0.1904140412393742\n",
      "2/100, error=0.08443746168326244\n",
      "3/100, error=0.06469184583177913\n",
      "4/100, error=0.05692088667162284\n",
      "5/100, error=0.04870857853292249\n",
      "6/100, error=0.042692483714764404\n",
      "7/100, error=0.04305004994587654\n",
      "8/100, error=0.039768175571013566\n",
      "9/100, error=0.04005235439076175\n",
      "10/100, error=0.03696713319602728\n",
      "11/100, error=0.03374244937117737\n",
      "12/100, error=0.03211482645141877\n",
      "13/100, error=0.029327654184351182\n",
      "14/100, error=0.02909696491791252\n",
      "15/100, error=0.02958812083174631\n",
      "16/100, error=0.02851921739323268\n",
      "17/100, error=0.023348829001051563\n",
      "18/100, error=0.026678822714901677\n",
      "19/100, error=0.02726098055756334\n",
      "20/100, error=0.025217623698102487\n",
      "21/100, error=0.024671114456522546\n",
      "22/100, error=0.025517934741243833\n",
      "23/100, error=0.025283755929077258\n",
      "24/100, error=0.02387131712158869\n",
      "25/100, error=0.022281336777385148\n",
      "26/100, error=0.023600828309355937\n",
      "27/100, error=0.02161376518016846\n",
      "28/100, error=0.020760396334214382\n",
      "29/100, error=0.021438469085464498\n",
      "30/100, error=0.01868048345496869\n",
      "31/100, error=0.01987736399190201\n",
      "32/100, error=0.020406011508614777\n",
      "33/100, error=0.020230987430073444\n",
      "34/100, error=0.020000662514504955\n",
      "35/100, error=0.020573220001548632\n",
      "36/100, error=0.02330779891961937\n",
      "37/100, error=0.019443270292094146\n",
      "38/100, error=0.019678877219631578\n",
      "39/100, error=0.01759703973527858\n",
      "40/100, error=0.018726446965662006\n",
      "41/100, error=0.023095429935670018\n",
      "42/100, error=0.020751991136639236\n",
      "43/100, error=0.015173837121085571\n",
      "44/100, error=0.016703829843468902\n",
      "45/100, error=0.018389517805705302\n",
      "46/100, error=0.01656124942923981\n",
      "47/100, error=0.018918332535272715\n",
      "48/100, error=0.016735527681919524\n",
      "49/100, error=0.016998077063285922\n",
      "50/100, error=0.015556764519043004\n",
      "51/100, error=0.015940072565694074\n",
      "52/100, error=0.01729074177604028\n",
      "53/100, error=0.01797524549107786\n",
      "54/100, error=0.015179793751804527\n",
      "55/100, error=0.016079585881573308\n",
      "56/100, error=0.015199223143917352\n",
      "57/100, error=0.017215203368802282\n",
      "58/100, error=0.01728018510123583\n",
      "59/100, error=0.016517572798792273\n",
      "60/100, error=0.01622296208557074\n",
      "61/100, error=0.015484724936002411\n",
      "62/100, error=0.01465441440834432\n",
      "63/100, error=0.01567669148217046\n",
      "64/100, error=0.01534067536321146\n",
      "65/100, error=0.01374466471554183\n",
      "66/100, error=0.01384735650126965\n",
      "67/100, error=0.015150146571815189\n",
      "68/100, error=0.01584011279415918\n",
      "69/100, error=0.013713517671528465\n",
      "70/100, error=0.016365696485991954\n",
      "71/100, error=0.01455904649739811\n",
      "72/100, error=0.015251448549375634\n",
      "73/100, error=0.015262201988331385\n",
      "74/100, error=0.01448196394641972\n",
      "75/100, error=0.014778101944002423\n",
      "76/100, error=0.01299833030116398\n",
      "77/100, error=0.013376774307716761\n",
      "78/100, error=0.013241668733094658\n",
      "79/100, error=0.013839003079664094\n",
      "80/100, error=0.013960865180850664\n",
      "81/100, error=0.012824292606561809\n",
      "82/100, error=0.013152061027542572\n",
      "83/100, error=0.014302590474100752\n",
      "84/100, error=0.013693710203697893\n",
      "85/100, error=0.01372863704470505\n",
      "86/100, error=0.012120016597805126\n",
      "87/100, error=0.012760447107187612\n",
      "88/100, error=0.012477618061798298\n",
      "89/100, error=0.013952057265556966\n",
      "90/100, error=0.01494613416800518\n",
      "91/100, error=0.015622144923364388\n",
      "92/100, error=0.0126221726358063\n",
      "93/100, error=0.013634646070502843\n",
      "94/100, error=0.01356896249205042\n",
      "95/100, error=0.013721483702758261\n",
      "96/100, error=0.012359367171207511\n",
      "97/100, error=0.013166424602493176\n",
      "98/100, error=0.011352413142058655\n",
      "99/100, error=0.01323985438393162\n",
      "100/100, error=0.012786877710439085\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "network = [\n",
    "    Convolutional((1, 28, 28), 3, 5),\n",
    "    Relu(),\n",
    "    Reshape((5, 26, 26), (5 * 26 * 26, 1)),\n",
    "    Dense(5 * 26 * 26, 100),\n",
    "    Relu(),\n",
    "    Dropout(0.05),\n",
    "    Dense(100, 10),  \n",
    "    Softmax() \n",
    "]\n",
    "\n",
    "# Train\n",
    "train(\n",
    "    network,\n",
    "    categorical_cross_entropy,\n",
    "    categorical_cross_entropy_prime,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    learning_rate= 0.1,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Neural Network on the Testing Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7, true: 7\n",
      "pred: 2, true: 2\n",
      "pred: 1, true: 1\n",
      "pred: 0, true: 0\n",
      "pred: 4, true: 4\n",
      "pred: 1, true: 1\n",
      "pred: 4, true: 4\n",
      "pred: 9, true: 9\n",
      "pred: 2, true: 5\n",
      "pred: 9, true: 9\n",
      "pred: 0, true: 0\n",
      "pred: 6, true: 6\n",
      "pred: 9, true: 9\n",
      "pred: 0, true: 0\n",
      "pred: 1, true: 1\n",
      "pred: 5, true: 5\n",
      "pred: 9, true: 9\n",
      "pred: 7, true: 7\n",
      "pred: 3, true: 3\n",
      "pred: 4, true: 4\n",
      "pred: 9, true: 9\n",
      "pred: 6, true: 6\n",
      "pred: 6, true: 6\n",
      "pred: 5, true: 5\n",
      "pred: 4, true: 4\n",
      "pred: 0, true: 0\n",
      "pred: 7, true: 7\n",
      "pred: 4, true: 4\n",
      "pred: 0, true: 0\n",
      "pred: 1, true: 1\n",
      "pred: 3, true: 3\n",
      "pred: 1, true: 1\n",
      "pred: 3, true: 3\n",
      "pred: 6, true: 4\n",
      "pred: 7, true: 7\n",
      "pred: 2, true: 2\n",
      "pred: 7, true: 7\n",
      "pred: 1, true: 1\n",
      "pred: 5, true: 2\n",
      "pred: 1, true: 1\n",
      "pred: 1, true: 1\n",
      "pred: 7, true: 7\n",
      "pred: 9, true: 4\n",
      "pred: 2, true: 2\n",
      "pred: 3, true: 3\n",
      "pred: 5, true: 5\n",
      "pred: 1, true: 1\n",
      "pred: 2, true: 2\n",
      "pred: 4, true: 4\n",
      "pred: 4, true: 4\n",
      "pred: 6, true: 6\n",
      "pred: 3, true: 3\n",
      "pred: 5, true: 5\n",
      "pred: 5, true: 5\n",
      "pred: 6, true: 6\n",
      "pred: 0, true: 0\n",
      "pred: 4, true: 4\n",
      "pred: 1, true: 1\n",
      "pred: 9, true: 9\n",
      "pred: 5, true: 5\n",
      "pred: 7, true: 7\n",
      "pred: 8, true: 8\n",
      "pred: 9, true: 9\n",
      "pred: 3, true: 3\n",
      "pred: 7, true: 7\n",
      "pred: 9, true: 4\n",
      "pred: 3, true: 6\n",
      "pred: 4, true: 4\n",
      "pred: 3, true: 3\n",
      "pred: 0, true: 0\n",
      "pred: 7, true: 7\n",
      "pred: 0, true: 0\n",
      "pred: 2, true: 2\n",
      "pred: 9, true: 9\n",
      "pred: 1, true: 1\n",
      "pred: 7, true: 7\n",
      "pred: 3, true: 3\n",
      "pred: 7, true: 2\n",
      "pred: 9, true: 9\n",
      "pred: 7, true: 7\n",
      "pred: 9, true: 7\n",
      "pred: 6, true: 6\n",
      "pred: 2, true: 2\n",
      "pred: 7, true: 7\n",
      "pred: 8, true: 8\n",
      "pred: 4, true: 4\n",
      "pred: 7, true: 7\n",
      "pred: 3, true: 3\n",
      "pred: 6, true: 6\n",
      "pred: 1, true: 1\n",
      "pred: 3, true: 3\n",
      "pred: 6, true: 6\n",
      "pred: 9, true: 9\n",
      "pred: 3, true: 3\n",
      "pred: 1, true: 1\n",
      "pred: 4, true: 4\n",
      "pred: 1, true: 1\n",
      "pred: 3, true: 7\n",
      "pred: 6, true: 6\n",
      "pred: 9, true: 9\n",
      "Correct: 91.0%\n"
     ]
    }
   ],
   "source": [
    "right = 0\n",
    "# Test\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x, False)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        right += 1\n",
    "    print(f\"pred: {np.argmax(output)}, true: {np.argmax(y)}\")\n",
    "\n",
    "total = (right / len(x_test)) * 100\n",
    "print(f\"Correct: {total}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
